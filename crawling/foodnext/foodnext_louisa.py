from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from bs4 import BeautifulSoup
import pymysql
import time
import requests

# è¨­ç½®ç›®æ¨™ç¶²å€
TARGET_URL = "https://www.foodnext.net/search"

# è¨­å®š Chrome WebDriver
options = webdriver.ChromeOptions()
options.add_argument("--headless")  # ç„¡é ­æ¨¡å¼ï¼ˆèƒŒæ™¯åŸ·è¡Œï¼‰
options.add_argument("--no-sandbox")
options.add_argument("--disable-dev-shm-usage")
options.add_argument("--disable-gpu")
options.add_argument("--remote-debugging-port=9222")

# å•Ÿå‹• WebDriver
service = Service()
driver = webdriver.Chrome(service=service, options=options)

# MySQL é€£ç·šå‡½å¼
def connect_to_db():
    try:
        conn = pymysql.connect(
            host="labdb.coded2.fun",
            user="sophia",
            password="123456dv107",
            db="SOPHIA",
            charset="utf8mb4"
        )
        return conn
    except pymysql.MySQLError as e:
        print(f"MySQL é€£ç·šéŒ¯èª¤: {e}")
        return None

# åˆå§‹åŒ– MySQL è³‡æ–™åº«
def setup_database():
    try:
        conn = connect_to_db()
        if not conn:
            print("ç„¡æ³•é€£æ¥åˆ°è³‡æ–™åº«")
            return
        
        cur = conn.cursor()
        # å‰µå»ºè³‡æ–™è¡¨ï¼Œç¢ºä¿ url æ˜¯å”¯ä¸€ç´¢å¼•ï¼Œé˜²æ­¢é‡è¤‡å„²å­˜
        cur.execute("""
            CREATE TABLE IF NOT EXISTS foodnext_louisa (
                id INT AUTO_INCREMENT PRIMARY KEY,
                title VARCHAR(255),
                url VARCHAR(255) UNIQUE,  
                date VARCHAR(255),
                content TEXT
            )
        """)
        conn.commit()
        conn.close()
        print("âœ… è³‡æ–™åº«åˆå§‹åŒ–å®Œæˆ")
    except pymysql.MySQLError as e:
        print(f"âŒ è¨­å®šè³‡æ–™åº«æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

# æ’å…¥æ–‡ç« è³‡æ–™ï¼ˆé˜²æ­¢é‡è¤‡å„²å­˜ï¼‰
def insert_article_data(data):
    conn = connect_to_db()
    if not conn:
        return
    
    cur = conn.cursor()
    try:
        for article in data:
            sql = """
                INSERT INTO foodnext_louisa (title, url, date, content)
                VALUES (%s, %s, %s, %s)
                ON DUPLICATE KEY UPDATE
                title = VALUES(title),
                date = VALUES(date),
                content = VALUES(content)
            """
            cur.execute(sql, (
                article['title'],
                article['url'],
                article['date'],
                "\n".join(article['relevant_contents'])
            ))
        conn.commit()
        print(f"âœ… æˆåŠŸæ’å…¥æˆ–æ›´æ–° {len(data)} ç¯‡æ–‡ç« è‡³è³‡æ–™åº«")
    except pymysql.MySQLError as e:
        print(f"âŒ æ’å…¥è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    finally:
        conn.close()

# å–å¾—æœå°‹çµæœä¸­çš„æ–‡ç« é€£çµ
def get_article_links():
    driver.get(TARGET_URL)
    print("âœ… Loaded page: " + driver.title)

    search_box = driver.find_element(By.CLASS_NAME, 'form-control')
    search_box.clear()
    search_box.send_keys("è·¯æ˜“è")
    search_box.submit()
    print("ğŸ” æœå°‹æˆåŠŸ")

    time.sleep(2)  # ç­‰å¾…æœå°‹çµæœè¼‰å…¥
    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
    time.sleep(2)

    soup = BeautifulSoup(driver.page_source, 'html.parser')
    results = soup.find_all('div', class_='search-result')

    article_links = []
    for result in results:
        link_tag = result.find('a')
        if link_tag and link_tag['href']:
            link = link_tag['href']
            if not link.startswith("http"):
                link = "https://www.foodnext.net" + link
            article_links.append(link)

    print(f"ğŸ“‘ æ‰¾åˆ° {len(article_links)} ç¯‡æ–‡ç« é€£çµ")
    return article_links

# çˆ¬å–æ–‡ç« å…§å®¹ä¸¦éæ¿¾å»¶ä¼¸é–±è®€
def fetch_article_content(article_links):
    article_data = []

    for link in article_links:
        try:
            response = requests.get(link, timeout=10)
            soup = BeautifulSoup(response.text, "html.parser")

            title_tag = soup.find('h3', class_='font-alt mt0')
            title = title_tag.text.strip() if title_tag else "æœªæ‰¾åˆ°æ¨™é¡Œ"

            date_tag = soup.find('p', class_='date')
            if not date_tag:
                date_span = soup.select_one('p.nm span.date')
                date = date_span.get_text(strip=True) if date_span else "æœªæ‰¾åˆ°æ—¥æœŸ"
            else:
                date = date_tag.get_text(strip=True)

            content_div = soup.find('div', class_='post-content')
            relevant_paragraphs = []
            if content_div:
                paragraphs = content_div.find_all('p')
                for paragraph in paragraphs:
                    text = paragraph.text.strip()
                    if "è·¯æ˜“è" in text and "â–¶" not in text and "å»¶ä¼¸é–±è®€" not in text:
                        relevant_paragraphs.append(text)

            if relevant_paragraphs:
                article_data.append({
                    "url": link,
                    "title": title,
                    "date": date,
                    "relevant_contents": relevant_paragraphs
                })
                print(f"ğŸ“ æ‰¾åˆ°ç›¸é—œå…§å®¹æ–¼ {link}")
                print(f"ğŸ“… æ—¥æœŸ: {date}")

        except requests.exceptions.RequestException as e:
            print(f"âŒ çˆ¬å– {link} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

    return article_data

# ä¸»ç¨‹å¼
try:
    setup_database()  # åˆå§‹åŒ–è³‡æ–™åº«
    article_links = get_article_links()  # å–å¾—æ–‡ç« é€£çµ
    article_data = fetch_article_content(article_links)  # çˆ¬å–å…§å®¹
    insert_article_data(article_data)  # å„²å­˜è‡³ MySQL
    print("âœ… æ‰€æœ‰è³‡æ–™å·²æˆåŠŸä¿å­˜")
except Exception as e:
    print(f"âŒ ç™¼ç”ŸéŒ¯èª¤: {e}")
finally:
    driver.quit()
