from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from bs4 import BeautifulSoup
from datetime import datetime
import pymysql
import time
import requests
import re

# è¨­ç½®ç›®æ¨™ç¶²å€
TARGET_URL = "https://www.foodnext.net/search"

# è¨­å®š Chrome WebDriver
options = webdriver.ChromeOptions()
options.add_argument("--headless")  # ç„¡é ­æ¨¡å¼ï¼ˆèƒŒæ™¯åŸ·è¡Œï¼‰
options.add_argument("--no-sandbox")
options.add_argument("--disable-dev-shm-usage")
options.add_argument("--disable-gpu")
options.add_argument("--remote-debugging-port=9222")

# å•Ÿå‹• WebDriver
service = Service()
driver = webdriver.Chrome(service=service, options=options)

# å®šç¾©åˆ†é¡è¦å‰‡
CLASSIFICATION_RULES = {
    "ç”¢å“ç›¸é—œé¡": r"æ–°å“|æ¨å‡º|å‡ç´š|æ–°å£æ„Ÿ|å…¨æ–°|æƒ…äººç¯€|æ–°ä¸Šå¸‚",
    "è¡ŒéŠ·æ´»å‹•é¡": r"å„ªæƒ |æŠ˜æ‰£|ä¿ƒéŠ·|å›é¥‹|ç‰¹åƒ¹|åŠåƒ¹|è²·ä¸€é€ä¸€|é™æ™‚|æœƒå“¡|é€ç¦®|ç¦®ç›’|é è³¼|è¯å|åˆä½œ|feat|è¯åˆ|æ”œæ‰‹|ç•°æ¥­çµç›Ÿ|ç­–ç•¥è¯ç›Ÿ|è·¨ç•Œ",
    "å“ç‰Œç™¼å±•é¡": r"å±•åº—|åŠ ç›Ÿ|æ‹“é»|æ–°åº—|æ——è‰¦åº—|é–€å¸‚|åˆ†åº—|æ“šé»|é–‹è¨­|é€²é§|æ“´å±•|æŠ•è³‡|ä½µè³¼|IPO|ä¸Šå¸‚|æ«ƒ|è‚¡æ±|è‚¡åƒ¹|å…¬é–‹ç™¼è¡Œ",
    "å¸‚å ´è¶¨å‹¢é¡": r"å¸‚å ´|ç”¢æ¥­è¶¨å‹¢|å¸‚ä½”ç‡|å•†æ©Ÿ|æˆé•·|è¶¨å‹¢|æœªä¾†å±•æœ›|ç­–ç•¥|å“ç‰Œå½¢è±¡|æ¶ˆè²»è¡Œç‚º|å¸‚å ´ç ”ç©¶|è©•é‘‘|å“ç‰Œç«¶çˆ­|å“ç‰Œé¸æ“‡",
    "å“ç‰Œé¡": r"æ˜Ÿå·´å…‹|ä¼¯æœ—|è¥¿é›…åœ–|è¶…å•†|æˆçœŸå’–å•¡|é»‘æ²ƒå’–å•¡|Flash Coffee|æ€¡å®¢å’–å•¡",
}

# è‡ªå‹•åˆ†é¡å‡½æ•¸
def classify_article(title, content):
    for category, pattern in CLASSIFICATION_RULES.items():
        if re.search(pattern, title, re.IGNORECASE) or re.search(pattern, content, re.IGNORECASE):
            return category
    return "æœªåˆ†é¡"

# è½‰æ›æ—¥æœŸæ ¼å¼
def format_date(date_str):
    """è‡ªå‹•åµæ¸¬ä¸¦è½‰æ› `YYYY/MM/DD` å’Œ `YYYY-MM-DD` æ ¼å¼"""
    try:
        if "/" in date_str:  # `2025/01/11`
            return datetime.strptime(date_str, "%Y/%m/%d").strftime("%Y-%m-%d")
        elif "-" in date_str:  # `2025-01-11`
            return datetime.strptime(date_str, "%Y-%m-%d").strftime("%Y-%m-%d")
        else:
            return None  # ä¸æ˜¯é æœŸæ ¼å¼æ™‚è¿”å› None
    except ValueError:
        return None

# MySQL é€£ç·šå‡½å¼
def connect_to_db():
    try:
        conn = pymysql.connect(
            host="è«‹å¡«å…¥ä½ çš„è³‡æ–™åº«ä¸»æ©ŸIP",
            user="è«‹å¡«å…¥ä½ çš„è³‡æ–™åº«å¸³è™Ÿ",
            password="è«‹å¡«å…¥ä½ çš„è³‡æ–™åº«å¯†ç¢¼",
            db="è«‹å¡«å…¥ä½ çš„è³‡æ–™åº«åç¨±",
            charset="utf8mb4"
        )
        return conn
    except pymysql.MySQLError as e:
        print(f"MySQL é€£ç·šéŒ¯èª¤: {e}")
        return None

# åˆå§‹åŒ– MySQL è³‡æ–™åº«
def setup_database():
    try:
        conn = connect_to_db()
        if not conn:
            print("ç„¡æ³•é€£æ¥åˆ°è³‡æ–™åº«")
            return
        
        cur = conn.cursor()
        # å‰µå»ºè³‡æ–™è¡¨ï¼Œç¢ºä¿ url æ˜¯å”¯ä¸€ç´¢å¼•ï¼Œé˜²æ­¢é‡è¤‡å„²å­˜
        cur.execute("""
            CREATE TABLE IF NOT EXISTS foodnext_cama (
                id INT AUTO_INCREMENT PRIMARY KEY,
                title VARCHAR(255),
                url VARCHAR(255) UNIQUE,  
                date DATE,
                content TEXT,
                classified VARCHAR(50)
            )
        """)
        conn.commit()
        conn.close()
        print("âœ… è³‡æ–™åº«åˆå§‹åŒ–å®Œæˆ")
    except pymysql.MySQLError as e:
        print(f"âŒ è¨­å®šè³‡æ–™åº«æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

# æ’å…¥æ–‡ç« è³‡æ–™ï¼ˆé˜²æ­¢é‡è¤‡å„²å­˜ï¼‰
def insert_article_data(data):
    conn = connect_to_db()
    if not conn:
        return
    
    cur = conn.cursor()
    try:
        for article in data:
            classified = classify_article(article['title'], " ".join(article['relevant_contents']))
            formatted_date = format_date(article['date'])
            if formatted_date is None:
                print(f"âš ï¸ æ—¥æœŸæ ¼å¼éŒ¯èª¤: {article['date']}ï¼Œè·³éæ­¤æ–‡ç« ")
                continue
            sql = """
                INSERT INTO foodnext_cama (title, url, date, content, classified)
                VALUES (%s, %s, %s, %s, %s)
                ON DUPLICATE KEY UPDATE
                title = VALUES(title),
                date = VALUES(date),
                content = VALUES(content),
                classified = VALUES(classified)
            """
            cur.execute(sql, (
                article['title'],
                article['url'],
                formatted_date,
                "\n".join(article['relevant_contents']),
                classified
            ))
        conn.commit()
        print(f"âœ… æˆåŠŸæ’å…¥æˆ–æ›´æ–° {len(data)} ç¯‡æ–‡ç« è‡³è³‡æ–™åº«")
    except pymysql.MySQLError as e:
        print(f"âŒ æ’å…¥è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    finally:
        conn.close()

# å–å¾—æœå°‹çµæœä¸­çš„æ–‡ç« é€£çµ
def get_article_links():
    driver.get(TARGET_URL)
    print("âœ… Loaded page: " + driver.title)

    search_box = driver.find_element(By.CLASS_NAME, 'form-control')
    search_box.clear()
    search_box.send_keys("cama")
    search_box.submit()
    print("ğŸ” æœå°‹æˆåŠŸ")

    time.sleep(2)  # ç­‰å¾…æœå°‹çµæœè¼‰å…¥
    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
    time.sleep(2)

    soup = BeautifulSoup(driver.page_source, 'html.parser')
    results = soup.find_all('div', class_='search-result')

    article_links = []
    for result in results:
        link_tag = result.find('a')
        if link_tag and link_tag['href']:
            link = link_tag['href']
            if not link.startswith("http"):
                link = "https://www.foodnext.net" + link
            article_links.append(link)

    print(f"ğŸ“‘ æ‰¾åˆ° {len(article_links)} ç¯‡æ–‡ç« é€£çµ")
    return article_links

def clean_content(text):
    # å®šç¾©éæ¿¾çš„æ­£å‰‡è¡¨é”å¼
    patterns = [
        r'ï¼ˆ?åœ–ç‰‡ä¾†æº[ï¼š:ï¼][^ï¼‰]*ï¼‰?',  # åŒ¹é… (åœ–ç‰‡ä¾†æºï¼šXXX) æˆ– åœ–ç‰‡ä¾†æºï¼šXXX
        r'ï¼ˆ?å…§å®¹æä¾›[ï¼š:ï¼][^ï¼‰]*ï¼‰?'  # åŒ¹é… (å…§å®¹æä¾›ï¼šXXX) æˆ– å…§å®¹æä¾›ï¼šXXX
    ]
    
    for pattern in patterns:
        text = re.sub(pattern, '', text).strip()  # ä½¿ç”¨æ­£å‰‡è¡¨é”å¼åˆªé™¤åŒ¹é…çš„å…§å®¹

    return text


# çˆ¬å–æ–‡ç« å…§å®¹ä¸¦éæ¿¾å»¶ä¼¸é–±è®€
def fetch_article_content(article_links):
    article_data = []

    for link in article_links:
        try:
            response = requests.get(link, timeout=10)
            soup = BeautifulSoup(response.text, "html.parser")

            title_tag = soup.find('h3', class_='font-alt mt0')
            title = title_tag.text.strip() if title_tag else "æœªæ‰¾åˆ°æ¨™é¡Œ"

            date_tag = soup.find('p', class_='date')
            if not date_tag:
                date_span = soup.select_one('p.nm span.date')
                date = date_span.get_text(strip=True) if date_span else "æœªæ‰¾åˆ°æ—¥æœŸ"
            else:
                date = date_tag.get_text(strip=True)

            formatted_date = format_date(date) if date else None
            if formatted_date is None:
                print(f"âš ï¸ æ—¥æœŸæ ¼å¼éŒ¯èª¤: {date}ï¼Œè·³éæ­¤æ–‡ç« ")
                continue  # ç›´æ¥è·³éé€™ç¯‡æ–‡ç« 

            content_div = soup.find('div', class_='post-content')
            relevant_paragraphs = []
            if content_div:
                paragraphs = content_div.find_all('p')
                for paragraph in paragraphs:
                    text = paragraph.text.strip()
                    text = clean_content(text)
                    if "cama" in text.lower() and "â–¶" not in text and "å»¶ä¼¸é–±è®€" not in text:
                        relevant_paragraphs.append(text)

            if relevant_paragraphs:
                article_data.append({
                    "url": link,
                    "title": title,
                    "date": formatted_date,
                    "relevant_contents": relevant_paragraphs
                })
                print(f"ğŸ“ æ‰¾åˆ°ç›¸é—œå…§å®¹æ–¼ {link}")
                print(f"ğŸ“… æ—¥æœŸ: {date}")

        except requests.exceptions.RequestException as e:
            print(f"âŒ çˆ¬å– {link} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

    return article_data

# ä¸»ç¨‹å¼
try:
    setup_database()  # åˆå§‹åŒ–è³‡æ–™åº«
    article_links = get_article_links()  # å–å¾—æ–‡ç« é€£çµ
    article_data = fetch_article_content(article_links)  # çˆ¬å–å…§å®¹
    insert_article_data(article_data)  # å„²å­˜è‡³ MySQL
    print("âœ… æ‰€æœ‰è³‡æ–™å·²æˆåŠŸä¿å­˜")
except Exception as e:
    print(f"âŒ ç™¼ç”ŸéŒ¯èª¤: {e}")
finally:
    driver.quit()
